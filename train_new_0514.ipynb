{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d5c461e-48cf-496e-9549-bd0ab461368a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ck696/.conda/envs/H3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/ck696/.conda/envs/H3/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowEllb\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import sys, os, torch\n",
    "from AllClear_v50_0514 import CogDataset_v46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0bcf3a1-f415-493c-867d-a8536f126dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(root='data', cloud_model_path='./data/Feature_Extrator_FS2.pth', save_model_path='./checkpoints29', dataset_name='AllClear_v1', load_gen='', load_dis='', n_epochs=100, gan_mode='lsgan', optimizer='AdamW', lr=0.0005, workers=4, batch_size=1, lambda_L1=100.0, lambda_aux=50.0, in_channel=4, out_channel=4, image_size=256, aux_loss=False, label_noise=False, gpu_id='3', manual_seed=2022)\n",
      "Load cloud_detection_model\n",
      "Load ours model\n",
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[0/100]:   0% 0/5000 [00:00<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate = 0.0005000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[0/100]:   1% 27/5000 [00:24<1:06:37,  1.24 step/s, D_fake=0.1624, D_real=0.3793, G_GAN=0.5167, G_L1=42.7302, G_L1_total=1818.9358]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 313\u001b[0m\n\u001b[1;32m    309\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m psnr, ssim\n\u001b[0;32m--> 313\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDIS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcloud_detection_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_G\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer_D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 229\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(opt, model_GEN, model_DIS, cloud_detection_model, optimizer_G, optimizer_D, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     loss_G \u001b[38;5;241m=\u001b[39m loss_G_GAN \u001b[38;5;241m+\u001b[39m loss_G_L1 \u001b[38;5;241m+\u001b[39m loss_g_att\n\u001b[0;32m--> 229\u001b[0m \u001b[43mloss_G\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m optimizer_G\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    232\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_G_GAN\u001b[39m\u001b[38;5;124m'\u001b[39m, loss_G_GAN, train_update)\n",
      "File \u001b[0;32m~/.conda/envs/H3/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/H3/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from utils import set_requires_grad, get_rgb, GANLoss\n",
    "from utils import *\n",
    "from tensorboardX import SummaryWriter\n",
    "import tqdm\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset_new import Sen2_MTC\n",
    "from model.fe import FeatureExtractor\n",
    "from model.pmaa import PMAA\n",
    "from model.discriminator import Discriminator\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\"\"\"Path\"\"\"\n",
    "parser.add_argument(\"--root\", type=str, default='data',\n",
    "                    help=\"Path to dataset\")\n",
    "parser.add_argument(\"--cloud_model_path\", type=str,\n",
    "                    default='./data/Feature_Extrator_FS2.pth', help=\"path to feature extractor model\")\n",
    "parser.add_argument(\"--save_model_path\", type=str,\n",
    "                    default='./checkpoints0514', help=\"Path to save model\")\n",
    "parser.add_argument(\"--dataset_name\", type=str, choices=[\"CTGAN_Sen2_MTC\", \"AllClear_v1\"],\n",
    "                    default='AllClear_v1', help=\"name of the dataset\")\n",
    "parser.add_argument(\"--load_gen\", type=str, default='',\n",
    "                    help=\"path to the model of generator\")\n",
    "parser.add_argument(\"--load_dis\", type=str, default='',\n",
    "                    help=\"path to the model of discriminator\")\n",
    "\n",
    "\"\"\"Parameters\"\"\"\n",
    "parser.add_argument(\"--n_epochs\", type=int,\n",
    "                    default=100, help=\"Number of epochs\")\n",
    "parser.add_argument(\"--gan_mode\", type=str, default='lsgan',\n",
    "                    help=\"Which gan mode(lsgan/vanilla)\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default='AdamW',\n",
    "                    help=\"optimizer you want to use(AdamW/SGD)\")\n",
    "parser.add_argument(\"--lr\", type=float, default=5e-4, help=\"learning rate\")\n",
    "parser.add_argument(\"--workers\", type=int, default=4,\n",
    "                    help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--batch_size\", type=int,\n",
    "                    default=1, help=\"size of the batches\")\n",
    "parser.add_argument('--lambda_L1', type=float,\n",
    "                    default=100.0, help='weight for L1 loss')\n",
    "parser.add_argument('--lambda_aux', type=float,\n",
    "                    default=50.0, help='weight for aux loss')\n",
    "parser.add_argument(\"--in_channel\", type=int, default=4,\n",
    "                    help=\"the number of input channels\")\n",
    "parser.add_argument(\"--out_channel\", type=int, default=4,\n",
    "                    help=\"the number of output channels\")\n",
    "parser.add_argument(\"--image_size\", type=int,\n",
    "                    default=256, help=\"crop size\")\n",
    "parser.add_argument(\"--aux_loss\", action='store_true',\n",
    "                    help=\"whether use auxiliary loss(1/0)\")\n",
    "parser.add_argument(\"--label_noise\", action='store_true',\n",
    "                    help=\"whether to add noise on the label of gan training\")\n",
    "\n",
    "\"\"\"base_options\"\"\"\n",
    "parser.add_argument(\"--gpu_id\", type=str, default='3', help=\"gpu id\")\n",
    "parser.add_argument(\"--manual_seed\", type=int,\n",
    "                    default=2022, help=\"random_seed you want\")\n",
    "\n",
    "opt, _ = parser.parse_known_args()\n",
    "print(opt)\n",
    "\n",
    "os.makedirs(os.path.join(opt.save_model_path,\n",
    "            opt.dataset_name), exist_ok=True)\n",
    "fixed_seed(opt.manual_seed)\n",
    "\n",
    "if opt.dataset_name == \"AllClear_v1\":\n",
    "    from AllClear_v50_0514 import CogDataset_v46\n",
    "    dataset = CogDataset_v46(max_num_frames=8, mode=\"train\", verbose=False)\n",
    "    train_loader = DataLoader(dataset, batch_size=opt.batch_size, shuffle=True, num_workers=opt.workers, drop_last=True)    \n",
    "    dataset = CogDataset_v46(max_num_frames=8, mode=\"val\", verbose=False)\n",
    "    val_loader = DataLoader(dataset, batch_size=opt.batch_size, shuffle=True, num_workers=opt.workers, drop_last=True)    \n",
    "    \n",
    "elif opt.dataset_name == \"CTGAN_Sen2_MTC\":\n",
    "    train_data = Sen2_MTC(opt, 'train')\n",
    "    val_data = Sen2_MTC(opt, mode='val')\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=opt.batch_size, shuffle=True,\n",
    "                              num_workers=opt.workers, drop_last=True, pin_memory=True, persistent_workers=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=opt.batch_size, shuffle=False,\n",
    "                            num_workers=opt.workers, drop_last=False, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "print('Load cloud_detection_model')\n",
    "cloud_detection_model = FeatureExtractor()\n",
    "cloud_detection_model.load_state_dict(torch.load(opt.cloud_model_path))\n",
    "cloud_detection_model.eval()\n",
    "set_requires_grad(cloud_detection_model, False)\n",
    "\n",
    "print('Load ours model')\n",
    "GEN = PMAA(32, 4)\n",
    "\n",
    "def replace_batchnorm(model):\n",
    "    for name, child in model.named_children():\n",
    "        if isinstance(child, torch.nn.BatchNorm2d):\n",
    "            child: torch.nn.BatchNorm2d = child\n",
    "            setattr(model, name, torch.nn.InstanceNorm2d(child.num_features))\n",
    "        else:\n",
    "            replace_batchnorm(child)\n",
    "replace_batchnorm(GEN)\n",
    "DIS = Discriminator()\n",
    "\n",
    "if opt.load_gen and opt.load_dis:\n",
    "    print('loading pre-trained model')\n",
    "    GEN.load_state_dict(torch.load(opt.load_gen))\n",
    "    DIS.load_state_dict(torch.load(opt.load_dis))\n",
    "\n",
    "if opt.optimizer == 'AdamW':\n",
    "    optimizer_G = torch.optim.AdamW(\n",
    "        GEN.parameters(), lr=opt.lr, betas=(0.5, 0.999), weight_decay=5e-4)\n",
    "    optimizer_D = torch.optim.AdamW(\n",
    "        DIS.parameters(), lr=opt.lr, betas=(0.5, 0.999), weight_decay=5e-4)\n",
    "if opt.optimizer == 'SGD':\n",
    "    optimizer_G = torch.optim.SGD(\n",
    "        GEN.parameters(), lr=opt.lr, momentum=0.9, nesterov=True)\n",
    "    optimizer_D = torch.optim.SGD(\n",
    "        DIS.parameters(), lr=opt.lr, momentum=0.9, nesterov=True)\n",
    "\n",
    "# def train(opt, model_GEN, model_DIS, cloud_detection_model, optimizer_G, optimizer_D, train_loader, val_loader):\n",
    "\n",
    "model_GEN = GEN\n",
    "model_DIS = DIS\n",
    "\n",
    "\n",
    "def train(opt, model_GEN, model_DIS, cloud_detection_model, optimizer_G, optimizer_D, train_loader, val_loader):\n",
    "    writer = SummaryWriter('runs29/%s' % opt.dataset_name)\n",
    "\n",
    "    noise = opt.label_noise\n",
    "    criterionGAN = GANLoss(opt.gan_mode)\n",
    "    criterionL1 = torch.nn.L1Loss()\n",
    "    criterionMSE = nn.MSELoss()\n",
    "\n",
    "    if cuda:\n",
    "        criterionGAN = criterionGAN.cuda()\n",
    "        criterionL1 = criterionL1.cuda()\n",
    "        criterionMSE = criterionMSE.cuda()\n",
    "        cloud_detection_model = cloud_detection_model.cuda()\n",
    "        model_GEN = model_GEN.cuda()\n",
    "        model_DIS = model_DIS.cuda()\n",
    "\n",
    "    \"\"\"lr_scheduler\"\"\"\n",
    "    scheduler_G = CosineAnnealingLR(\n",
    "        optimizer_G, T_max=opt.n_epochs, eta_min=1e-6)\n",
    "    scheduler_D = CosineAnnealingLR(\n",
    "        optimizer_D, T_max=opt.n_epochs, eta_min=1e-6)\n",
    "\n",
    "    \"\"\"training\"\"\"\n",
    "    train_update = 0\n",
    "    psnr_max = 0.\n",
    "    ssim_max = 0.\n",
    "\n",
    "    print('Start training!')\n",
    "    for epoch in range(opt.n_epochs):\n",
    "        model_GEN.train()\n",
    "        model_DIS.train()\n",
    "\n",
    "        pbar = tqdm.tqdm(total=len(train_loader), ncols=0,\n",
    "                         desc=\"Train[%d/%d]\" % (epoch, opt.n_epochs), unit=\" step\")\n",
    "\n",
    "        lr = optimizer_G.param_groups[0]['lr']\n",
    "        print('\\nlearning rate = %.7f' % lr)\n",
    "\n",
    "        L1_total = 0\n",
    "        for real_A, real_B, _ in train_loader:\n",
    "            real_A[0], real_A[1], real_A[2], real_B = real_A[0].cuda(\n",
    "            ), real_A[1].cuda(), real_A[2].cuda(), real_B.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                M0, _, _ = cloud_detection_model(real_A[0])\n",
    "                M1, _, _ = cloud_detection_model(real_A[1])\n",
    "                M2, _, _ = cloud_detection_model(real_A[2])\n",
    "            M = [M0, M1, M2]\n",
    "\n",
    "            real_A_combined = torch.cat(\n",
    "                (real_A[0], real_A[1], real_A[2]), 1).cuda()\n",
    "            real_A_input = torch.stack(\n",
    "                (real_A[0], real_A[1], real_A[2]), 1).cuda()\n",
    "\n",
    "            \"\"\"forward generator\"\"\"\n",
    "            fake_B, cloud_mask, aux_pred = model_GEN(real_A_input)\n",
    "\n",
    "            \"\"\"update Discriminator\"\"\"\n",
    "            set_requires_grad(model_DIS, True)\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            fake_AB = torch.cat((real_A_combined, fake_B), 1)\n",
    "            pred_fake = model_DIS(fake_AB.detach())\n",
    "            loss_D_fake = criterionGAN(pred_fake, False, noise)\n",
    "\n",
    "            real_AB = torch.cat((real_A_combined, real_B), 1)\n",
    "            pred_real = model_DIS(real_AB)\n",
    "            loss_D_real = criterionGAN(pred_real, True, noise)\n",
    "\n",
    "            loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            \"\"\"update generator\"\"\"\n",
    "            optimizer_G.zero_grad()\n",
    "            set_requires_grad(model_DIS, False)\n",
    "\n",
    "            fake_AB = torch.cat((real_A_combined, fake_B), 1)\n",
    "            pred_fake = model_DIS(fake_AB)\n",
    "            loss_G_GAN = criterionGAN(pred_fake, True, noise)\n",
    "\n",
    "            loss_G_L1 = criterionL1(fake_B, real_B) * opt.lambda_L1\n",
    "            L1_total += loss_G_L1.item()\n",
    "\n",
    "            loss_g_att = 0\n",
    "            for i in range(len(cloud_mask)):\n",
    "                loss_g_att += criterionMSE(cloud_mask[i]\n",
    "                                           [:, 0, :, :], M[i][:, 0, :, :])\n",
    "\n",
    "            if opt.aux_loss:\n",
    "                loss_G_aux = (criterionL1(aux_pred[0], real_B) + criterionL1(\n",
    "                    aux_pred[1], real_B) + criterionL1(aux_pred[2], real_B)) * opt.lambda_aux\n",
    "                loss_G = loss_G_GAN + loss_G_L1 + loss_g_att + loss_G_aux\n",
    "            else:\n",
    "                loss_G = loss_G_GAN + loss_G_L1 + loss_g_att\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            writer.add_scalar('training_G_GAN', loss_G_GAN, train_update)\n",
    "            writer.add_scalar('training_G_L1', loss_G_L1, train_update)\n",
    "            writer.add_scalar('training_D_real', loss_D_real, train_update)\n",
    "            writer.add_scalar('training_D_fake', loss_D_fake, train_update)\n",
    "            writer.add_scalar('training_D_fake', loss_g_att, train_update)\n",
    "\n",
    "            pbar.update()\n",
    "            pbar.set_postfix(\n",
    "                G_GAN=f\"{loss_G_GAN:.4f}\",\n",
    "                G_L1=f\"{loss_G_L1:.4f}\",\n",
    "                G_L1_total=f\"{L1_total:.4f}\",\n",
    "                D_real=f\"{loss_D_real:.4f}\",\n",
    "                D_fake=f\"{loss_D_fake:.4f}\"\n",
    "            )\n",
    "            train_update += 1\n",
    "        pbar.close()\n",
    "        \"\"\"validation\"\"\"\n",
    "        psnr, ssim = valid(opt, model_GEN, val_loader,\n",
    "                           criterionL1, writer, epoch)\n",
    "\n",
    "        if psnr_max < psnr:\n",
    "            psnr_max = psnr\n",
    "            torch.save(model_GEN.state_dict(), os.path.join(\n",
    "                opt.save_model_path, opt.dataset_name, f'G_best_PSNR_{psnr:.3f}_SSIM_{ssim:.3f}.pth'))\n",
    "\n",
    "        if ssim_max < ssim:\n",
    "            ssim_max = ssim\n",
    "            torch.save(model_GEN.state_dict(), os.path.join(\n",
    "                opt.save_model_path, opt.dataset_name, f'G_best_SSIM_{ssim:.3f}_PNSR_{psnr:.3f}.pth'))\n",
    "\n",
    "        scheduler_D.step()\n",
    "        scheduler_G.step()\n",
    "\n",
    "    print('Best PSNR: %.3f | Best SSIM: %.3f' % (psnr_max, ssim_max))\n",
    "\n",
    "def valid(opt, model_GEN, val_loader, criterionL1, writer, epoch):\n",
    "    model_GEN.eval()\n",
    "\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm.tqdm(total=len(val_loader), ncols=0,\n",
    "                     desc=\"Valid[%d/%d]\" % (epoch, opt.n_epochs), unit=\" step\")\n",
    "    with torch.no_grad():\n",
    "        for (real_A, real_B, image_names) in val_loader:\n",
    "            real_A[0], real_A[1], real_A[2], real_B = real_A[0].cuda(\n",
    "            ), real_A[1].cuda(), real_A[2].cuda(), real_B.cuda()\n",
    "            real_A_input = torch.stack(\n",
    "                (real_A[0], real_A[1], real_A[2]), 1).cuda()\n",
    "            fake_B, _, _ = model_GEN(real_A_input)\n",
    "\n",
    "            loss = criterionL1(fake_B, real_B)\n",
    "\n",
    "            for batch in range(len(image_names)):\n",
    "                output, label = fake_B[batch], real_B[batch]\n",
    "                output_rgb, label_rgb = get_rgb(output), get_rgb(label)\n",
    "\n",
    "                psnr, ssim = psnr_ssim_cal(label_rgb, output_rgb)\n",
    "                psnr_list.append(psnr)\n",
    "                ssim_list.append(ssim)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.update()\n",
    "            pbar.set_postfix(\n",
    "                loss_val=f\"{total_loss:.4f}\"\n",
    "            )\n",
    "    psnr_list = np.array(psnr_list)\n",
    "    ssim_list = np.array(ssim_list)\n",
    "    psnr = np.mean(psnr_list)\n",
    "    ssim = np.mean(ssim_list)\n",
    "\n",
    "    writer.add_scalar('validation_PSNR', psnr, epoch)\n",
    "    writer.add_scalar('validation_SSIM', ssim, epoch)\n",
    "    pbar.set_postfix(loss_val=f\"{total_loss:.4f}\",\n",
    "                     psnr=f\"{psnr:.3f}\", ssim=f\"{ssim:.3f}\")\n",
    "\n",
    "    pbar.close()\n",
    "    return psnr, ssim\n",
    "\n",
    "\n",
    "train(opt, GEN, DIS, cloud_detection_model, optimizer_G,\n",
    "      optimizer_D, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddafe7-6b41-48a9-b609-f3cb59a9e460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a16a42-be9f-4d34-9624-f6a9518eba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359ea9a1-ffa4-495c-8743-95b0971c06c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c64cb6-7955-4b86-96eb-74348894c8a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7b7d83-5b0d-480f-8077-778a64749f35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for real_A, real_B, _ in train_loader:\n",
    "    real_A[0], real_A[1], real_A[2], real_B = real_A[0].cuda(\n",
    "    ), real_A[1].cuda(), real_A[2].cuda(), real_B.cuda()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4603256-9e25-4f88-83b2-001d3533f01d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "real_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5084f86-e4b9-48e0-8d6d-889e7ba70db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return three  where \n",
    "real_A[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
